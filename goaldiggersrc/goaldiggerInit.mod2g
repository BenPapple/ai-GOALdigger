use goaldiggerProlog as knowledge.

/**
 * File initializes first knowledge into agent, gets executed once at agent launch
 *
 */

module goaldiggerInit {
	
	% debug and custom competition rules
	if true then insert (lDebugOn). % Variable f√ºr Debug, zB elapseStepTime
	%if true then insert (activateDoubleSpeed). % switch to deactivate because not allowed in competition1
	% To count sim changes
	if not(bel(simCount(_))) then insert(simCount(0)). 
	if bel(simCount(SimCount), NewSimCount is SimCount + 1) then delete(simCount(SimCount)) + insert(simCount(NewSimCount)). 
	
	% global values
	if true then insert(expectDifferentSimulations). % switch to recognize simulations have different characteristics on sim change
	if true then insert(limitChangeStepMinMax(10 , 20)). % lowest and highest limit after which agent changes explore direction
	
	% switches
	if true then insert (haveMove). % Semaphore of decision for step
	if true then insert (haveBlockAttached(false, n)). % switch for carried blocks
	if true then insert (haveDispenserDelivery(false, 0)). % switch for successful dispenser request
	
	% Assign Explorer and set worker as default role
	if bel(randomNumber(Seed)) then insert(randomSeed(Seed)).
	if true then insert (positionInHirarchie(0)).
	if true then insert (targetRole(worker)).

	% subscribing to message channels
	if true then subscribe(channelGoalzone).
	if true then subscribe(channelDispenser).
	if true then subscribe(channelNamesRelativeXY).
	if true then subscribe(channnelMultiTask).
	
	% dispenser targets
	if true then insert (targetDispenserAt(70, 70, b9, 700)). % init with dummy values 
	if true then insert (targetClosestOfAllDispensersAt(70, 70, b9, 700)). % init with dummy values 
	
	% goalzone targets
	if true then insert (targetClosestGoalZone(111, 111, 123456)). % init with dummy values 	
	
	% tasks
	if true then insert(currentChosenTask(task0, -11001001, 10 , 0, 1, b0, placeholder, placeholder2)). % filler task -11001001 so it gets replaced when real task available
		
	% manhattan distance
	if true then insert (targetMd(-999, -999, placeholdertarget)). % init Manhatten Distance 
	if true then insert (nMd(0)). % ManhattanDistance for field north of agent
	if true then insert (sMd(0)). % ManhattanDistance for field south of agent
	if true then insert (eMd(0)). % ManhattanDistance for field east of agent
	if true then insert (wMd(0)). % ManhattanDistance for field west of agent

	% misc
	if true then insert ( step(-1)). % init server steps
	if true then insert (agentAt(0,0)). % agent starting coordinate	
	if bel(randomDirection(X)) then insert (randomAffinity(X)). % random general direction to explore for agent
	if bel(limitChangeStepMinMax(X, Y), randomBetween(X, Y, Out)) then insert (changeAffinityAfterTheseSteps(Out)). % self explaining
	if true then insert (elapseStepTime(0)). % Time measurement between step
	if true then insert (skipThisStep(-2)). % turns to skip with random explore move
	
	% variables for world size calculation
	if true then insert ( worldSizeX(54321), worldSizeY(54321) ).
	if true then insert ( worldListX([]), worldListY([]) ).
	if true then insert ( messageProcessingDelay(4), messagePersitanceAfterDelay(1) ).
	if percept(team(Team)) then insert(ownTeam(Team)). %The agent learns its own team from the initial percept.
	if percept(name(Name)) then insert ( ownName(Name) ). % TO DO: I have observed this doesn't get inserted in most cases. Why?
}